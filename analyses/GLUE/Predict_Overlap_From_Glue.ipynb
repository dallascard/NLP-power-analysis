{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "colab_type": "code",
    "id": "rd8GWMVq22P6",
    "outputId": "b23d76d1-5715-4929-8ae1-4294db36f93d"
   },
   "outputs": [],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "\n",
    "# This will prompt for authorization.\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "6TMkI9Bv2_Dk",
    "outputId": "b7b4d30d-2a51-4dd7-cb70-d336ffb06258"
   },
   "outputs": [],
   "source": [
    "!ls \"/content/drive/My Drive/NLP Power Analysis/data/glue_submissoins\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G3W42Ykt9E0h"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "glue_leaderboard = pd.read_html(\"/content/drive/My Drive/NLP Power Analysis/data/glue_submissoins/GLUE Benchmark.html\", header=0)[0]\n",
    "glue_leaderboard[\"Model\"] = glue_leaderboard[\"Model\"].apply(lambda x: x.replace(\"Microsoft Research Paraphrase Corpus-F1 / Accuracy\",\"\").lower().strip())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "HysFupzq4PU4",
    "outputId": "4e3e985d-719d-4ae9-d8fa-bb1677e6fe83"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "base_path = \"/content/drive/My Drive/NLP Power Analysis/data/glue_submissoins\"\n",
    "models = [x for x in os.listdir(base_path) if not x.endswith(\".html\")]\n",
    "print(models)\n",
    "tasks = os.listdir(os.path.join(base_path, models[0]))\n",
    "print(tasks)\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "pairwise = list(combinations(models, 2))\n",
    "task = \"MRPC.tsv\"\n",
    "\n",
    "mapping = {\n",
    "    \"electra_small\" : \"ELECTRA-Small\",\n",
    "    \"electra_base\" : \"ELECTRA-Base\",\n",
    "    \"electra_large\" : \"ELECTRA-Large\",\n",
    "    \"electra_large_tricks\" : \"ELECTRA-Large + Standard Tricks\",\n",
    "    \"albert\" : \"Albert (Ensemble)\",\n",
    "    \"XLNET\" : \"XlNet (ensemble)\",\n",
    "    \"BAM\" : \"BERT + BAM\",\n",
    "    \"BERT\" : \"BERT: 24-layers, 16-heads, 1024-hidden\"\n",
    "}\n",
    "\n",
    "def _get_table_key(modelname):\n",
    "  if modelname in mapping.keys():\n",
    "    return mapping[modelname].lower()\n",
    "  else:\n",
    "    return modelname.lower()\n",
    "\n",
    "\n",
    "x = []\n",
    "texts=[]\n",
    "y = []\n",
    "for model1, model2 in pairwise:\n",
    "  model1_preds = pd.read_csv(os.path.join(base_path, model1, task), delimiter=\"\\t\").sort_values(by=\"index\").reset_index(drop=True)\n",
    "  model2_preds = pd.read_csv(os.path.join(base_path, model2, task), delimiter=\"\\t\").sort_values(by=\"index\").reset_index(drop=True)\n",
    "  \n",
    "  # print(model1_preds)\n",
    "  # print(model1_preds.iloc[:,1])\n",
    "  print(f\"{model1} v. {model2}\")\n",
    "\n",
    "  if \"MRPC\" in task or \"QQP\" in task:\n",
    "    model1_performance = float(glue_leaderboard[glue_leaderboard[\"Model\"] == _get_table_key(model1)][task.replace(\".tsv\", \"\")].values[0].split(\"/\")[1])\n",
    "    model2_performance = float(glue_leaderboard[glue_leaderboard[\"Model\"] == _get_table_key(model2)][task.replace(\".tsv\", \"\")].values[0].split(\"/\")[1])\n",
    "  else:\n",
    "    model1_performance = float(glue_leaderboard[glue_leaderboard[\"Model\"] == _get_table_key(model1)][task.replace(\".tsv\", \"\")].values[0])\n",
    "    model2_performance = float(glue_leaderboard[glue_leaderboard[\"Model\"] == _get_table_key(model2)][task.replace(\".tsv\", \"\")].values[0])  \n",
    "  y.append((model1_preds.iloc[:,1] == model2_preds.iloc[:,1]).mean())\n",
    "\n",
    "  print(f\"min performance: {model1_performance}\")\n",
    "  x.append(min(model1_performance, model2_performance))\n",
    "  texts.append(f\"{model1} v. {model2}\")\n",
    "  print(f\"overlap: {y[-1]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "qv52RLCFBYsX",
    "outputId": "e828b6f5-7a48-4f71-88c2-76fa5788f93a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# !pip install adjustText\n",
    "from adjustText import adjust_text\n",
    "import seaborn as sns\n",
    "\n",
    "for task in tasks:\n",
    "  print(task)\n",
    "  x = []\n",
    "  texts=[]\n",
    "  y = []\n",
    "  for model1, model2 in pairwise:\n",
    "    model1_preds = pd.read_csv(os.path.join(base_path, model1, task), delimiter=\"\\t\").sort_values(by=\"index\").reset_index(drop=True)\n",
    "    model2_preds = pd.read_csv(os.path.join(base_path, model2, task), delimiter=\"\\t\").sort_values(by=\"index\").reset_index(drop=True)\n",
    "    \n",
    "    # print(model1_preds)\n",
    "    # print(model1_preds.iloc[:,1])\n",
    "    print(f\"{model1} v. {model2}\")\n",
    "\n",
    "    if \"MRPC\" in task or \"QQP\" in task:\n",
    "      model1_performance = float(glue_leaderboard[glue_leaderboard[\"Model\"] == _get_table_key(model1)][task.replace(\".tsv\", \"\")].values[0].split(\"/\")[1])\n",
    "      model2_performance = float(glue_leaderboard[glue_leaderboard[\"Model\"] == _get_table_key(model2)][task.replace(\".tsv\", \"\")].values[0].split(\"/\")[1])\n",
    "    else:\n",
    "      try:\n",
    "        model1_performance = float(glue_leaderboard[glue_leaderboard[\"Model\"] == _get_table_key(model1)][task.replace(\".tsv\", \"\")].values[0])\n",
    "        model2_performance = float(glue_leaderboard[glue_leaderboard[\"Model\"] == _get_table_key(model2)][task.replace(\".tsv\", \"\")].values[0]) \n",
    "      except:\n",
    "        print(f\"Issue parsing model {model2} on {task}.\")\n",
    "        continue\n",
    "    y.append((model1_preds.iloc[:,1] == model2_preds.iloc[:,1]).mean())\n",
    "\n",
    "    print(f\"min performance: {model1_performance}\")\n",
    "    x.append(min(model1_performance, model2_performance))\n",
    "    texts.append(f\"{model1} v. {model2}\")\n",
    "    print(f\"overlap: {y[-1]}\")\n",
    "\n",
    "\n",
    "  plt.figure()\n",
    "\n",
    "  sns.set(color_codes=True, font_scale=1.5)\n",
    "  fig, ax = plt.subplots(figsize=(20,20))\n",
    "  plt.title(task.replace(\".tsv\",\"\"))\n",
    "  plt.xlabel(\"Min. Accuracy\")\n",
    "  plt.ylabel(\"Percent Overlap\")\n",
    "  ax.scatter(x, y)\n",
    "  annotations = []\n",
    "  for i, txt in enumerate(texts):\n",
    "      annotations.append(ax.annotate(txt, (x[i], y[i])))\n",
    "\n",
    "  print(adjust_text(annotations, y=y, x=x, ax=ax, arrowprops=dict(arrowstyle=\"->\", color='r', lw=1.0), expand_points=(1.3, 1.2), expand_text=(1.3, 1.2), precision=0.001, force_text= (1.1, .6), force_points=(1.0, 1.0), force_objects=(1.0, 0.6)))\n",
    "\n",
    "  plt.savefig(f'/content/drive/My Drive/NLP Power Analysis/plots/glue/percent_overlap_glue_{task.replace(\".tsv\",\"\").lower()}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z-Ju1BiUkI5E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 842
    },
    "colab_type": "code",
    "id": "B_pWhY-wHQlS",
    "outputId": "b61075d3-99af-45e5-8505-6b91ef40379a"
   },
   "outputs": [],
   "source": [
    "# Do the above for all tasks\n",
    "\n",
    "tasks_to_cover = [\"MRPC.tsv\", \"QQP.tsv\", \"SST-2.tsv\", \"RTE.tsv\", \"MNLI-m.tsv\", \"MNLI-mm.tsv\", \"WNLI.tsv\"]\n",
    "\n",
    "def _get_table_key(modelname):\n",
    "  if modelname in mapping.keys():\n",
    "    return mapping[modelname].lower()\n",
    "  else:\n",
    "    return modelname.lower()\n",
    "\n",
    "\n",
    "x = []\n",
    "texts=[]\n",
    "y = []\n",
    "for task in tasks_to_cover:\n",
    "  for model1, model2 in pairwise:\n",
    "    if task == \"WNLI.tsv\" and (\"electra\" in model1.lower() or \"electra\" in model2.lower() or \"bert\" in model1.lower() or \"bert\" in model2.lower() or \"bam\" in model1.lower() or \"bam\" in model2.lower()):\n",
    "      print(f\"skipping {task} {model1} {model2}\")\n",
    "      continue\n",
    "    model1_preds = pd.read_csv(os.path.join(base_path, model1, task), delimiter=\"\\t\").sort_values(by=\"index\").reset_index(drop=True)\n",
    "    model2_preds = pd.read_csv(os.path.join(base_path, model2, task), delimiter=\"\\t\").sort_values(by=\"index\").reset_index(drop=True)\n",
    "    \n",
    "    # print(model1_preds)\n",
    "    # print(model1_preds.iloc[:,1])\n",
    "\n",
    "    if \"MRPC\" in task or \"QQP\" in task:\n",
    "      model1_performance = float(glue_leaderboard[glue_leaderboard[\"Model\"] == _get_table_key(model1)][task.replace(\".tsv\", \"\")].values[0].split(\"/\")[1]) / 100.0\n",
    "      model2_performance = float(glue_leaderboard[glue_leaderboard[\"Model\"] == _get_table_key(model2)][task.replace(\".tsv\", \"\")].values[0].split(\"/\")[1]) / 100.0\n",
    "    else:\n",
    "      model1_performance = float(glue_leaderboard[glue_leaderboard[\"Model\"] == _get_table_key(model1)][task.replace(\".tsv\", \"\")].values[0]) / 100.0\n",
    "      model2_performance = float(glue_leaderboard[glue_leaderboard[\"Model\"] == _get_table_key(model2)][task.replace(\".tsv\", \"\")].values[0]) / 100.0\n",
    "    y.append((model1_preds.iloc[:,1] == model2_preds.iloc[:,1]).mean())\n",
    "    min_performer = min(model1_performance, model2_performance)\n",
    "    max_perfromer = max(model1_performance, model2_performance)\n",
    "    x.append([min_performer, max_perfromer - min_performer])\n",
    "    texts.append(f\"{task.replace('.tsv', '')} {model1} v. {model2}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "3Td-nOPrHsJs",
    "outputId": "dcd5f032-f56b-4d84-f857-4e3757024fe0"
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm     \n",
    "import numpy as np \n",
    "from scipy import stats\n",
    "\n",
    "x = np.array(x)\n",
    "print(x.shape)\n",
    "\n",
    "xt = sm.add_constant(x)\n",
    "# import pdb; pdb.set_trace()\n",
    "# xt = np.array(x)\n",
    "lm_1 = sm.OLS(np.array(y), xt).fit()\n",
    "print(lm_1.summary().as_latex())\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "sns.set(color_codes=True, font_scale=1.5, style='white')\n",
    "# sns.set(font_scale=20)  # crazy big\n",
    "\n",
    "ax = sns.regplot(x=x[:,0] * 100.0, y=y, label=\"test\")\n",
    "# ax = p.axes[0]\n",
    "ax.legend()\n",
    "leg = ax.get_legend()\n",
    "L_labels = leg.get_texts()\n",
    "# assuming you computed r_squared which is the coefficient of determination somewhere else\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x[:,0], y)\n",
    "label_line_2 = r'$R^2:{0:.5f}$'.format(r_value) # as an exampple or whatever you want[!\n",
    "L_labels[0].set_text(label_line_2)\n",
    "# ax.tick_params\n",
    "ax.set_xlabel(\"Baseline Accuracy\")\n",
    "ax.set_ylabel(\"Test % Agreement\")\n",
    "\n",
    "plt.savefig('/content/drive/My Drive/NLP Power Analysis/plots/glue/glue_baseline_v_disagreement.pdf', bbox_inches='tight', \n",
    "               transparent=True,\n",
    "               pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "whBDjq3NnW7a",
    "outputId": "a8b6536f-7c48-4e09-ab7e-3615cd6e0d9f"
   },
   "outputs": [],
   "source": [
    "print(lm_1.params)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Predict Overlap From Glue",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
