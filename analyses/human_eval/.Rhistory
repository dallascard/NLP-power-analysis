nsim = num.iter,
family = "gaussian",
newdata = expdat,
newparams = list(theta = theta, beta = beta, sigma=sigma),
REML=F)
# assign responses
expdat$resp <- ss[, 1]
print(nrow(expdat))
# fit a first model
fit1 <- lmer(resp ~ model + (1 + model || worker) + (1 + model || item),
data = expdat, REML=F)
# fit a model for num.iter times, return time t value is greater than 1.96
fitsim <- function(i) {
coef(summary(refit(fit1, ss[[i]])))["model", ]
}
fitAll <- t(data.frame(sapply(seq(num.iter), function(i) fitsim(i))))
return(mean(fitAll[, 't value'] > 1.96))
}
###########
################# lmer with low var parameters
lmer.overall.lowvar = NULL
for (diff in c(.05, .1, .15, .2)) {
for (ex in c(50, 100, 500))  {
for (num.workers in c(3, 10)) {
power = simulate_power_params(200, num.workers, ex, num.workers, .5, diff, .13, .01, .04, .01, .16)
print(c(power, diff))
lmer.overall.lowvar = rbind(lmer.overall.lowvar, cbind(diff, ex, power, num.workers))
}
}
}
library(tidyverse)
library(lme4)
library(xtable)
library(here)
here()
here()
processed_dir = here('data')
processed_dir
# HUSE data
d.lm = read_csv("processed_data/huse_lm.csv") %>%
mutate(resp = response_val/max(response_val),
model=model_code) %>%
rename(item=input_id)
# HUSE data
d.lm = read_csv(file.path(processed_dir, "huse_lm.csv")) %>%
mutate(resp = response_val/max(response_val),
model=model_code) %>%
rename(item=input_id)
group_by(d.lm, model_code) %>%
summarise(m=mean(resp), sd=sd(resp))
d.lm$model = as.factor(as.numeric(as.factor(d.lm$model)))
contrasts(d.lm$model) = contr.sum(length(unique(d.lm$model)))
l.lm = d.lm %>%
lmer(data=.,
resp ~ model + (model||item) + (model||worker),
REML=F)
summary(l.lm)
# WMT model
d.wmt = read_csv(file.path(processed_dir, "wmt19.csv")) %>%
mutate(resp=score_scaled,
model=model_code) %>%
rename(worker=WorkerId,
item=sid)
processed_dir = here('data', 'annotated_datasets')
# WMT model
d.wmt = read_csv(file.path(processed_dir, "wmt19.csv")) %>%
mutate(resp=score_scaled,
model=model_code) %>%
rename(worker=WorkerId,
item=sid)
# WMT model
d.wmt = read_csv(file.path(processed_dir, "wmt19.csv")) %>%
mutate(resp=score_scaled,
model=model_code) %>%
rename(worker=WorkerId,
item=sid)
d.wmt$model = as.factor(as.numeric(as.factor(d.wmt$model)))
contrasts(d.wmt$model) = contr.sum(2)
l.wmt = d.wmt %>%
lmer(data=.,
resp ~ model + (model||item) + (model||worker),
REML=F)
summary(l.wmt)
# Uber data
d.uber = read_csv(file.path(processed_dir, "uber_all.csv")) %>%
mutate(resp=rating_scaled) %>%
rename(worker=annotator,
item=item_id)
d.uber$model = as.factor(as.numeric(as.factor(d.uber$model)))
contrasts(d.uber$model) = contr.sum(length(unique(d.uber$model)))
group_by(d.uber, model) %>%
summarise(m=mean(resp), sd=sd(resp))
l.uber = d.uber %>%
lmer(data=.,
resp ~ model + (model||item) + (model||worker),
REML=F)
summary(l.uber)
# Uber data
#d.uber = read_csv(file.path(processed_dir, "uber_all.csv")) %>%
d.uber = read_csv(file.path('..', '..', 'human-eval-analysis', 'processed_data', "uber_all.csv")) %>%
mutate(resp=rating_scaled) %>%
rename(worker=annotator,
item=item_id)
# Uber data
#d.uber = read_csv(file.path(processed_dir, "uber_all.csv")) %>%
d.uber = read_csv(file.path('/Users', 'dallas', 'Projects', 'stanford', 'human-eval-analysis', 'processed_data', "uber_all.csv")) %>%
mutate(resp=rating_scaled) %>%
rename(worker=annotator,
item=item_id)
d.uber$model = as.factor(as.numeric(as.factor(d.uber$model)))
contrasts(d.uber$model) = contr.sum(length(unique(d.uber$model)))
group_by(d.uber, model) %>%
summarise(m=mean(resp), sd=sd(resp))
l.uber = d.uber %>%
lmer(data=.,
resp ~ model + (model||item) + (model||worker),
REML=F)
summary(l.uber)
# Uber data
#d.uber = read_csv(file.path('/Users', 'dallas', 'Projects', 'stanford', 'human-eval-analysis', 'processed_data', "uber_all.csv")) %>%
d.uber = read_csv(file.path(processed_dir, "uber_all.csv")) %>%
mutate(resp=rating_scaled,
model=model_code) %>%
rename(worker=annotator,
item=item_id)
d.uber$model = as.factor(as.numeric(as.factor(d.uber$model)))
contrasts(d.uber$model) = contr.sum(length(unique(d.uber$model)))
group_by(d.uber, model) %>%
summarise(m=mean(resp), sd=sd(resp))
l.uber = d.uber %>%
lmer(data=.,
resp ~ model + (model||item) + (model||worker),
REML=F)
summary(l.uber)
# reddit data
d.reddit.all = read_csv(file.path(processed_dir, "huse_reddit.csv")) %>%
mutate(response_val = response_val/max(response_val),
item = input_id)
# reddit data
d.reddit.all = read_csv(file.path(processed_dir, "huse_reddit.csv")) %>%
mutate(response_val = response_val/max(response_val),
item = input_id)
d.reddit.all$model = as.factor(as.numeric(as.factor(d.reddit.all$model)))
contrasts(d.reddit.all$model) = contr.sum(length(unique(d.reddit.all$model)))
l.reddit.all = d.reddit.all %>%  lmer(data=.,
response_val ~ model + (model||item) + (model||worker),
REML=F)
summary(l.reddit.all)
# summarization data
d.summarization = read_csv(file.path(processed_dir, "summarization.csv")) %>%
mutate(response_val = response_val/max(response_val),
item = input_id)
# summarization data
d.summarization = read_csv(file.path(processed_dir, "huse_summarization.csv")) %>%
mutate(response_val = response_val/max(response_val),
item = input_id)
d.summarization$model = as.factor(as.numeric(as.factor(d.summarization$model)))
contrasts(d.summarization$model) = contr.sum(length(unique(d.summarization$model)))
l.summarization =  d.summarization %>%
lmer(data=.,
response_val ~ model + (model||item) + (model||worker),
REML=F)
summary(l.summarization)
### Commented out because data is not public
d.nucleus = read_csv("processed_data/nucleus.csv") %>%
mutate(resp=response_val/max(response_val),
model=model_id,
item=text_id)
### Commented out because data is not public
d.nucleus = read_csv(file.path(processed_dir, "nucleus.csv")) %>%
mutate(resp=response_val/max(response_val),
model=model_id,
item=text_id)
d.nucleus$model = as.factor(as.numeric(as.factor(d.nucleus$model)))
contrasts(d.nucleus$model) = contr.sum(length(unique(d.nucleus$model)))
l.nucleus = d.nucleus %>%
lmer(data=.,
resp ~ model + (model|item) + (model|worker),
REML=F)
summary(l.nucleus)
summary(l.nucleus)
summary(l.nucleus)
l.nucleus = d.nucleus %>%
lmer(data=.,
resp ~ model + (model|item) + (model|worker),
REML=F)
summary(l.nucleus)
library(tidyverse)
# overall, not just likert
s = read_csv("humaneval_master.csv")
# make a summary data frame
d.summary = data.frame(rbind(cbind("huse.reddit", length(unique(d.reddit.all$worker)),
length(unique(d.reddit.all$item))),
#cbind("nucleus", length(unique(d.nucleus$worker_id)),
#length(unique(d.nucleus$item))),
cbind("wmt", length(unique(d.wmt$worker)),
length(unique(d.wmt$item))),
cbind("uber", length(unique(d.uber$worker)),
length(unique(d.uber$item))),
cbind("huse.lm", length(unique(d.lm$worker)),
length(unique(d.lm$item))),
cbind("huse.summarization", length(unique(d.summarization$worker)),
length(unique(d.summarization$item)))))
# make a summary data frame
d.summary = data.frame(rbind(cbind("huse.reddit", length(unique(d.reddit.all$worker)),
length(unique(d.reddit.all$item))),
#cbind("nucleus", length(unique(d.nucleus$worker_id)),
#length(unique(d.nucleus$item))),
cbind("wmt", length(unique(d.wmt$worker)),
length(unique(d.wmt$item))),
cbind("uber", length(unique(d.uber$worker)),
length(unique(d.uber$item))),
cbind("huse.lm", length(unique(d.lm$worker)),
length(unique(d.lm$item))),
cbind("huse.summarization", length(unique(d.summarization$worker)),
length(unique(d.summarization$item)))))
names(d.summary) = c("dataset", "num.workers", "num.items")
xtable(d.summary)
### aggregate parameters into a single data frame, to output nice latex tables
a = NULL
ls = c(l.nucleus, l.wmt, l.uber, l.lm, l.reddit.all, l.summarization)
#ls = c(l.nucleus, l.wmt, l.uber, l.lm, l.reddit.all, l.summarization)
ls = c(l.wmt, l.uber, l.lm, l.reddit.all, l.summarization)
#lnames = c("nucleus", "wmt", "uber", "huse.lm", "huse.reddit", "huse.sum")
lnames = c("wmt", "uber", "huse.lm", "huse.reddit", "huse.sum")
for (l in 1:length(ls))  {
a = bind_rows(a, bind_rows(tibble(sdcor = as.numeric(fixef(ls[[l]])),
grp=names(fixef(ls[[l]])),
var1 = "fixed.effect"),
data.frame(VarCorr(ls[[l]])) %>%
filter(is.na(var2)) %>%
select(grp, var1, sdcor)) %>%
mutate(model = as.character(lnames[l])))
}
a$var1 = gsub("fixed.effect", "$\\\\beta$", a$var1)
a$var = paste(a$grp, a$var1, sep=".")
a$var = gsub("Residual.NA", "sigma", a$var)
a = select(a, dataset=model, var, value=sdcor)
a$var = gsub("\\(", "", a$var)
a$var = gsub("\\)", "", a$var)
# get fixed effect df
fe = filter(a, grepl("eta", var) | grepl("sigma", var)) %>% mutate(value = round(value, 2))
# get random effect df
re = filter(a, !grepl("beta", var)) %>% mutate(value = round(value, 2))
re = mutate(re, var = gsub("1\\.", "", var))
re = mutate(re, var = gsub("model", "", var))
re = mutate(re, var = gsub("Intercept", "int", var))
# Fixed effects, Table 13
spread(fe, var, value, fill="") %>%
mutate(index = c(1, 2, 1, 6, 2, 1)) %>%
arrange(index) %>%
select( -index) %>%
rename(`$\\sigma$`=sigma) %>% #intercept=Intercept.beta, model1.beta, model2.beta, model3.beta, model4.beta, model5.beta, model6.beta) %>%
xtable(caption="Fixed effect coefficients for each model along with the residual model variance.", label="table:fixef") %>%
print(include.rownames=FALSE, sanitize.colnames.function = identity)
# Fixed effects, Table 13
spread(fe, var, value, fill="") %>%
#mutate(index = c(1, 2, 1, 6, 2, 1)) %>%
mutate(index = c(2, 1, 6, 2, 1)) %>%
arrange(index) %>%
select( -index) %>%
rename(`$\\sigma$`=sigma) %>% #intercept=Intercept.beta, model1.beta, model2.beta, model3.beta, model4.beta, model5.beta, model6.beta) %>%
xtable(caption="Fixed effect coefficients for each model along with the residual model variance.", label="table:fixef") %>%
print(include.rownames=FALSE, sanitize.colnames.function = identity)
worker
# Random effects for worker, Table 14
spread(mutate(re, var=gsub("worker\\.", "model ", var)), var, value, fill="") %>%
#mutate(index = c(1, 2, 1, 6, 2, 1)) %>%
mutate(index = c(2, 1, 6, 2, 1)) %>%
arrange(index) %>%
select(dataset, `model int`, `model 1`:`model 6`) %>%
xtable(caption="Random effects standard deviations for worker. worker.int is the intercept and the rest of the parameters are slopes for each model.",
label="table:worker") %>%
print(include.rownames=FALSE)
# Random effects for worker, Table 14
spread(mutate(re, var=gsub("worker\\.", "model ", var)), var, value, fill="") %>%
#mutate(index = c(1, 2, 1, 6, 2, 1)) %>%
mutate(index = c(2, 1, 6, 2, 1)) %>%
arrange(index) %>%
select(dataset, `model int`, `model 1`:`model 5`) %>%
xtable(caption="Random effects standard deviations for worker. worker.int is the intercept and the rest of the parameters are slopes for each model.",
label="table:worker") %>%
print(include.rownames=FALSE)
# Random effects for worker, Table 14
spread(mutate(re, var=gsub("worker\\.", "model ", var)), var, value, fill="") %>%
#mutate(index = c(1, 2, 1, 6, 2, 1)) %>%
mutate(index = c(2, 1, 6, 2, 1)) %>%
arrange(index) %>%
select(dataset, `model int`, `model 1`:`model 5`) %>%
xtable(caption="Random effects standard deviations for worker. worker.int is the intercept and the rest of the parameters are slopes for each model.",
label="table:worker") %>%
print(include.rownames=FALSE)
# Random effects for worker, Table 14
spread(mutate(re, var=gsub("worker\\.", "model ", var)), var, value, fill="") %>%
#mutate(index = c(1, 2, 1, 6, 2, 1)) %>%
mutate(index = c(2, 1, 6, 2, 1)) %>%
arrange(index) %>%
select(dataset, `model int`, `model 1`:`model 4`) %>%
xtable(caption="Random effects standard deviations for worker. worker.int is the intercept and the rest of the parameters are slopes for each model.",
label="table:worker") %>%
print(include.rownames=FALSE)
# Random effects for item, Table 15
spread(mutate(re, var=gsub("item\\.", "model ", var)), var, value, fill="") %>%
#mutate(index = c(1, 2, 1, 6, 2, 1)) %>%
mutate(index = c(2, 1, 6, 2, 1)) %>%
arrange(index) %>%
select(dataset, `model int`, `model 1`:`model 4`) %>%
xtable(caption="Random effects standard deviations for item item.int is the intercept and the rest of the parameters are slopes for each model.",
label="table:item") %>%
print(include.rownames=FALSE)
# Try to use here() to specify the right directory
# This should return the project directroy/data/human_eval/
processed_dir = here('data', 'human_eval')
processed_dir
# overall, not just likert
s = read_csv("humaneval_master.csv")
# overall, not just likert
s = read_csv(file.path(data_dir, "humaneval_master.csv"))
# Try to use here() to specify the right directory
# This should return the project directroy/data/human_eval/
data_dir = here('data', 'human_eval')
data_dir
# overall, not just likert
s = read_csv(file.path(data_dir, "humaneval_master.csv"))
print(length(unique(s$Link)))
s.new = filter(s, `Probably not relevant to us` == 0)
print(length(unique(s.new$Link)))
print(nrow(s.new))
# mean with significance test
s.new$HasTest = as.numeric(s.new$HasTest)
mean(s.new$HasTest)
# print by response type
table(s.new$ResponseCategory)
# print bold
s.new[is.na(s.new$`Bold text in results table`), "Bold text in results table"] = "no"
mean(s.new$`Bold text in results table` == "yes")
s1 = read_csv(file.path(data_dir, "likert_ratings.csv"))
e = s1 %>%
mutate(Response = gsub("likert", "Likert", `Response`)) %>%
separate(Response, into=c("likert_min", "likert_max"), sep="_") %>%
mutate(likert_max = as.numeric(likert_max),
likert_min = as.numeric(likert_min) ,
Value = as.numeric(Value),
score = (Value - likert_min)/likert_max,
Category = gsub("[0-9]", "", Category)) %>%
filter(is.na(Value) == F) %>%
group_by(Link, `Eval number`, Dataset, Metric, Category, Examples, Ann.Per.Ex, Tot.Ann) %>%
summarise(max.in.cat = max(score)) %>%
ungroup() %>%
mutate(id = as.integer(as.factor(paste(Link, `Eval number`, Dataset, Metric))))
e$Category = tolower(e$Category)
baseline.new = filter(e, Category %in% c("baseline", "new")) %>%
spread(Category, max.in.cat) %>%
mutate(diff = new - baseline,
diffPos = diff > 0) %>%
arrange(diff) %>%
mutate(diffrank = 1:n())
metrics = read.csv("metrics.csv")
metrics = read.csv(file.path(data_dir, "metrics.csv"))
metrics = read.csv(file.path(data_dir, "metrics.csv"))
baseline.new = left_join(baseline.new, metrics)
baseline.new$diffrank = as.factor(baseline.new$diffrank)
# filter those that don't have at least one baseline and one new
baseline.new = filter(baseline.new, is.na(diff) == F)
ggplot(filter(baseline.new,
is.na(baseline) == F,
baseline != Inf),
aes(x=diffrank, y=new)) +  geom_point(colour="red") +
geom_point(aes(x=diffrank, y=baseline, colour="black"), colour="black") +
geom_segment(aes(x=diffrank, xend = diffrank, y = baseline, yend=new),  arrow = arrow(length = unit(0.4,"cm"))) +
ggtitle("Likert ratings, change from best baseline to best new") +
xlab("Model comparison") +
ylab("normalized Likert rating") +
theme_bw(14) +
facet_grid(. ~ BigCategory, scales = "free_x")
ggsave("likert_comparisons.pdf", width=7, height=7)
length(unique(baseline.new$Link))
nrow(baseline.new)
filter(baseline.new, is.na(Examples) == F) %>%
summarise(
e.100 = sum(Examples<= 100)/n(),
e.200 = sum(Examples == 200)/n(),
e.morethan200 = sum(Examples > 200)/n())
# cor test for publication bias
cor.test(baseline.new$diff, baseline.new$Examples, method="kendall")
# print what's missing and median experimental characteristics
mean(is.na(baseline.new$Ann.Per.Ex))
mean(is.na(baseline.new$Examples))
mean(is.na(baseline.new$Tot.Ann))
mean(!is.na(baseline.new$Tot.Ann) & !is.na(baseline.new$Ann.Per.Ex) & !is.na(baseline.new$Examples))
median(baseline.new$Tot.Ann, na.rm=T)
median(baseline.new$Ann.Per.Ex, na.rm=T)
mean(baseline.new$Ann.Per.Ex == 3, na.rm=T)
# make plot number of items vs effect size
ggplot(baseline.new, aes(x=Examples, y=diff)) +
theme_bw(12) +
xlab("number of items") +
ylab("effect size") +
ylim(-.5, .5) + geom_jitter( size=3,alpha=.4) + scale_x_log10() +
geom_smooth(method=lm) +
geom_hline(yintercept=0, colour='red')
ggsave("items_vs_effect_size.pdf", width=5, height=2.7)
library(lme4)
library(tidyverse)
# taken in part from https://rstudio-pubs-static.s3.amazonaws.com/11703_21d1c073558845e1b56ec921c6e0931e.html
simulate_power_params <- function(num.iter, nannotators, nitems, nperitem, intercept, beta,
item.model, item.Intercept, worker.model, worker.Intercept, sigma) {
# make a data set with a full factorial of annotators, items
# 2 conditions for model: 1 or 0
expdat <- expand.grid(worker = factor(1:nannotators),
item = factor(1:nitems),
model = c(-1, 1)
) %>%
group_by(item, model) %>%
sample_n(nperitem)
set.seed(101)
beta <- c(intercept, beta) # intercept effect
names(beta) = c("(Intercept)", "model")
# specify sigma and theta in sd space
theta <- c(item.model, item.Intercept, worker.model, worker.Intercept) #item.model, item.Intercept, worker.model, worker.Intercept
names(theta) = c("item.model", "item.(Intercept)", "worker.model", "worker.(Intercept)")
theta = theta/sigma # theta is specified scaled by sigma, so we divide to get it in the right space
# simulate
ss <- simulate(~model + (1 + model || worker) + (1 + model || item),
nsim = num.iter,
family = "gaussian",
newdata = expdat,
newparams = list(theta = theta, beta = beta, sigma=sigma),
REML=F)
# assign responses
expdat$resp <- ss[, 1]
print(nrow(expdat))
# fit a first model
fit1 <- lmer(resp ~ model + (1 + model || worker) + (1 + model || item),
data = expdat, REML=F)
# fit a model for num.iter times, return time t value is greater than 1.96
fitsim <- function(i) {
coef(summary(refit(fit1, ss[[i]])))["model", ]
}
fitAll <- t(data.frame(sapply(seq(num.iter), function(i) fitsim(i))))
return(mean(fitAll[, 't value'] > 1.96))
}
###########
################# lmer with low var parameters
lmer.overall.lowvar = NULL
(lme4)
library(lme4)
library(tidyverse)
library(here)
source(here("estimate_parameters_from_datasets.R"))
# This should return the project directory/analyses/human_eval/estimate_parameters_from_datasets.R
infile = here('analyses', 'human_eval', 'estimate_parameters_from_datasets.R')
infile
library(lme4)
library(tidyverse)
library(here)
# This should return the project directory/analyses/human_eval/estimate_parameters_from_datasets.R
infile = here('analyses', 'human_eval', 'estimate_parameters_from_datasets.R')
infile
source(infile)
library(lme4)
library(tidyverse)
# taken in part from https://rstudio-pubs-static.s3.amazonaws.com/11703_21d1c073558845e1b56ec921c6e0931e.html
simulate_power_params <- function(num.iter, nannotators, nitems, nperitem, intercept, beta,
item.model, item.Intercept, worker.model, worker.Intercept, sigma) {
# make a data set with a full factorial of annotators, items
# 2 conditions for model: 1 or 0
expdat <- expand.grid(worker = factor(1:nannotators),
item = factor(1:nitems),
model = c(-1, 1)
) %>%
group_by(item, model) %>%
sample_n(nperitem)
set.seed(101)
beta <- c(intercept, beta) # intercept effect
names(beta) = c("(Intercept)", "model")
# specify sigma and theta in sd space
theta <- c(item.model, item.Intercept, worker.model, worker.Intercept) #item.model, item.Intercept, worker.model, worker.Intercept
names(theta) = c("item.model", "item.(Intercept)", "worker.model", "worker.(Intercept)")
theta = theta/sigma # theta is specified scaled by sigma, so we divide to get it in the right space
# simulate
ss <- simulate(~model + (1 + model || worker) + (1 + model || item),
nsim = num.iter,
family = "gaussian",
newdata = expdat,
newparams = list(theta = theta, beta = beta, sigma=sigma),
REML=F)
# assign responses
expdat$resp <- ss[, 1]
print(nrow(expdat))
# fit a first model
fit1 <- lmer(resp ~ model + (1 + model || worker) + (1 + model || item),
data = expdat, REML=F)
# fit a model for num.iter times, return time t value is greater than 1.96
fitsim <- function(i) {
coef(summary(refit(fit1, ss[[i]])))["model", ]
}
fitAll <- t(data.frame(sapply(seq(num.iter), function(i) fitsim(i))))
return(mean(fitAll[, 't value'] > 1.96))
}
###########
################# lmer with low var parameters
lmer.overall.lowvar = NULL
for (diff in c(.05, .1, .15, .2)) {
for (ex in c(50, 100, 500))  {
for (num.workers in c(3, 10)) {
power = simulate_power_params(200, num.workers, ex, num.workers, .5, diff, .13, .01, .04, .01, .16)
print(c(power, diff))
lmer.overall.lowvar = rbind(lmer.overall.lowvar, cbind(diff, ex, power, num.workers))
}
}
}
